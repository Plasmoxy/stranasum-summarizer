{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sebastian Petrik - Stranasum - Development module\n",
    "\n",
    "STranASum - Simple transformer abstractive summarization.\n",
    "Development module of the implementation of the abstractive summarization project for my bachelor thesis.\n",
    "\n",
    "Inspired by https://www.tensorflow.org/text/tutorials/transformer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python -m pip install --upgrade -q wandb --quiet\n",
    "!python -m pip install evaluate rouge-score contractions --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.environ.get('KAGGLE_CONTAINER_NAME')) # check if kaggle\n",
    "print(os.environ.get('PATH'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "sorted(list(filter(\n",
    "    lambda x: x[0] in ['matplotlib', 'contractions', 'numpy', 'pandas', 'tensorflow', 'tensorflow-text', 'keras', 'tensorflow-estimator', 'tensorflow-datasets', 'wandb', 'evaluate', 'rouge_score', 'gensim'],\n",
    "    [(i.key, i.version) for i in pkg_resources.working_set]\n",
    ")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import operator as op\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "from pprint import pprint\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup seeds\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1' \n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = dict(\n",
    "    # Meta\n",
    "    wandb_project = 'stranasum-tune',\n",
    "    wandb_group = 'gigaword',\n",
    "    host = 'local',\n",
    "    \n",
    "    # Data - use dataset name with dataset directory\n",
    "    # dataset_name = \"pmxy-stranasum-data/xsum_10-150_3-40_v0.05_t0.05\",\n",
    "    # dataset_name = \"pmxy-stranasum-data/inshorts_10-70_3-16_v0.05_t0.05\",\n",
    "    # dataset_name = \"pmxy-stranasum-data-combined/combined_10-150_3-40_v0.05_t0.05\",\n",
    "    dataset_name = \"stranasum-gigaword-70-to-25/gigaword\",\n",
    "    \n",
    "    # Sequences\n",
    "    # - define lengths according to data\n",
    "    max_input_length = 70, # Encoder sequence length, max article len, max token count in Encoder.\n",
    "    max_target_length = 25, # Decoder sequence length, max summary len, max token count in Decoder.\n",
    "    tokenizer_mode = 'separate', # single or separate\n",
    "    \n",
    "    # Embeddings - remember to set correct d_model !!!\n",
    "    embedding_mode = 'glove6b100d', # normal, glove840b300d, glove6b100d and alternatives\n",
    "    embeddings_trainable = False, # ! affects only pretrained embeddings !\n",
    "\n",
    "    # Transformer hyperparameters\n",
    "    num_layers = 3, # 4\n",
    "    d_model = 100, # 128 -> embedding length ~ dimensions\n",
    "    dff = 512, # 512\n",
    "    num_heads = 8, # 8\n",
    "    dropout_rate = 0.1, # 0.1\n",
    "\n",
    "    # Training\n",
    "    early_stopping_patience = 2, # patience - num of non-improving consecutive epochs\n",
    "    max_epochs = 3, # 40\n",
    "    batch_size = 410, # 256 default, 512 ok for g6b100d\n",
    "    learning_rate_warmup_steps = 4000 # 4000\n",
    ")\n",
    "\n",
    "print(\"Config:\")\n",
    "pprint(CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "if CONFIG['host'] == 'kaggle':\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    wandb.login(key=user_secrets.get_secret(\"wandb\"))\n",
    "else:\n",
    "    wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- load data from preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show available data\n",
    "!ls ../input\n",
    "!ls ../input/pmxy-stranasum-preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"../input/\"\n",
    "df_train = pd.read_csv(f\"{dataset_dir}/{CONFIG['dataset_name']}_train.csv\")\n",
    "df_val = pd.read_csv(f\"{dataset_dir}/{CONFIG['dataset_name']}_val.csv\")\n",
    "df_test = pd.read_csv(f\"{dataset_dir}/{CONFIG['dataset_name']}_test.csv\")\n",
    "\n",
    "print(\"Train:\", df_train.shape)\n",
    "print(\"Val:\", df_val.shape)\n",
    "print(\"Test:\", df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom tokenizer\n",
    "class Tokenizer(tf.Module):\n",
    "    \n",
    "    def __init__(self, vectorization_dataset: tf.data.Dataset, max_length: int):\n",
    "        super().__init__(name=\"Tokenizer\")\n",
    "        \n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Create and fit vectorizer from dataset\n",
    "        self.vectorizer = tf.keras.layers.TextVectorization(\n",
    "            output_mode='int',\n",
    "            output_sequence_length=self.max_length,\n",
    "            standardize=None\n",
    "        )\n",
    "        self.vectorizer.adapt(vectorization_dataset.batch(1024))\n",
    "        \n",
    "        # conversions\n",
    "        self.word_to_id = tf.keras.layers.StringLookup(\n",
    "            vocabulary=self.vectorizer.get_vocabulary(),\n",
    "            mask_token='', oov_token='[UNK]'\n",
    "        )\n",
    "        self.id_to_word = tf.keras.layers.StringLookup(\n",
    "            vocabulary=self.vectorizer.get_vocabulary(),\n",
    "            mask_token='', oov_token='[UNK]',\n",
    "            invert=True\n",
    "        )\n",
    "        \n",
    "        # attributes\n",
    "        self.vocab_size = self.vectorizer.vocabulary_size()\n",
    "        self.start_token = self.word_to_id('<sos>')\n",
    "        self.end_token = self.word_to_id('<eos>')\n",
    "        \n",
    "        print(f\"Tokenizer maxlen={self.max_length}, top vocabulary: {self.vectorizer.get_vocabulary()[:10]}\")\n",
    "        \n",
    "        \n",
    "    # to convert text to tokens, call vectorizer directly !\n",
    "        \n",
    "    # convert tokens back to text\n",
    "    @tf.function\n",
    "    def tokens_to_text(self, tokens):\n",
    "        words = self.id_to_word(tokens)\n",
    "        result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
    "        result = tf.strings.regex_replace(result, '^ *<sos> *', '')\n",
    "        result = tf.strings.regex_replace(result, ' *<eos> *$', '')\n",
    "        result = tf.strings.regex_replace(result, '<dot>', '.')\n",
    "        return result\n",
    "\n",
    "# Setup tokenizers\n",
    "\n",
    "input_tokenizer = None\n",
    "target_tokenizer = None\n",
    "\n",
    "if CONFIG['tokenizer_mode'] == 'single':\n",
    "    tok = Tokenizer(\n",
    "        tf.data.Dataset.from_tensor_slices(df_train['article']).concatenate(tf.data.Dataset.from_tensor_slices(df_train['summary'])),\n",
    "        max(CONFIG['max_input_length'], CONFIG['max_target_length'])\n",
    "    )\n",
    "    input_tokenizer = tok\n",
    "    target_tokenizer = tok\n",
    "else:\n",
    "    input_tokenizer = Tokenizer(\n",
    "        tf.data.Dataset.from_tensor_slices(df_train['article']),\n",
    "        CONFIG['max_input_length']\n",
    "    )\n",
    "\n",
    "    target_tokenizer = Tokenizer(\n",
    "        tf.data.Dataset.from_tensor_slices(df_train['summary']),\n",
    "        CONFIG['max_target_length']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try out \n",
    "sample_tokens = input_tokenizer.vectorizer(tf.constant(['<sos> the dog ate the food <eos>']))\n",
    "sample_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokenizer.tokens_to_text(sample_tokens).numpy().astype('str') # binary string tensor into decoded string array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_tokenizer.vectorizer(tf.constant('<sos> a man was murdered <eos>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph function for vectorization and shifted label creation\n",
    "# x: Tensor (batch, 2)\n",
    "# for each batch we have input and target string in the tensor\n",
    "def transform_to_tokenized_sequences(x: tf.Tensor):\n",
    "    # take batch of input strings, vectorize them\n",
    "    inputs = input_tokenizer.vectorizer(x[:,0])\n",
    "    \n",
    "    # take batch of output strings, vectorize them\n",
    "    targets = target_tokenizer.vectorizer(x[:,1])\n",
    "    \n",
    "    # targets are present as variable length (Ragged) Tensors per batch member ...\n",
    "    \n",
    "    # drop their EOS tokens at the end\n",
    "    targets_inputs = targets[:,:-1]\n",
    "    \n",
    "    # drop SOS token, shifting sequence 1 step behind providing next word labels for each step\n",
    "    targets_labels = targets[:,1:]\n",
    "    \n",
    "    # final output feedable to the Transformer\n",
    "    return (inputs, targets_inputs), targets_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tokenization and create batched tf datasets ...\n",
    "\n",
    "# convert articles and summaries into tensorflow dataset\n",
    "# shuffle - shuffle with buffer size = size of data for full uniform shuffle\n",
    "# attach a mapper to map texts into token tensors with shifted target labels\n",
    "\n",
    "dataset_train = tf.data.Dataset.from_tensor_slices(\n",
    "    # convert_to_tokenized_sequences(df_train['article'], df_train['summary'])\n",
    "    df_train[['article', 'summary']],\n",
    ").shuffle(df_train.shape[0]).batch(CONFIG['batch_size']).map(\n",
    "    transform_to_tokenized_sequences\n",
    ")\n",
    "\n",
    "dataset_val = tf.data.Dataset.from_tensor_slices(\n",
    "    # convert_to_tokenized_sequences(df_val['article'], df_val['summary'])\n",
    "    df_val[['article', 'summary']],\n",
    ").shuffle(df_val.shape[0]).batch(CONFIG['batch_size']).map(\n",
    "    transform_to_tokenized_sequences\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take 1 sample batch for further inspection (later)\n",
    "for (sample_input, sample_target), sample_target_labels in dataset_train.take(1):\n",
    "    break\n",
    "    \n",
    "print('Sample single batch from dataset:')\n",
    "\n",
    "print(sample_input.shape)\n",
    "print(sample_input[0])\n",
    "print(sample_target.shape)\n",
    "print(sample_target_labels.shape)\n",
    "\n",
    "print(sample_target[0])\n",
    "print(sample_target_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to get special token numbers so we can see them in prints\n",
    "print(input_tokenizer.vectorizer(tf.constant('<sos> <eos> <unk> [UNK] <dot> <pad>')))\n",
    "print(target_tokenizer.vectorizer(tf.constant('<sos> <eos> <unk> [UNK] <dot> <pad>')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Legacy)\n",
    "# glove_vectors = gensim.downloader.load('glove-twitter-25')\n",
    "# glove_vectors.most_similar('queen')\n",
    "# glove_vectors.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb_matrix(tokenizer, embedding_vectors):\n",
    "    hits = 0\n",
    "    misses = 0\n",
    "    embedding_dim = CONFIG['d_model']\n",
    "\n",
    "    tokenizer_vocabulary = tokenizer.vectorizer.get_vocabulary()\n",
    "    embedding_matrix = np.zeros((tokenizer.vocab_size, embedding_dim))\n",
    "\n",
    "    for i in tqdm(range(tokenizer.vocab_size)):\n",
    "        embedding_val = embedding_vectors.get(tokenizer_vocabulary[i])\n",
    "\n",
    "        if embedding_val is not None:\n",
    "            embedding_matrix[i] = embedding_val\n",
    "            hits += 1\n",
    "        else:\n",
    "            misses += 1\n",
    "\n",
    "    print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_emb_matrix = None\n",
    "target_emb_matrix = None\n",
    "\n",
    "if CONFIG['embedding_mode'] != 'normal':\n",
    "    print(\"Setting up glove pretrained embeddings...\")\n",
    "    \n",
    "    embedding_vectors = {}\n",
    "    \n",
    "    path = None\n",
    "    if CONFIG['embedding_mode'] == 'glove840b300d':\n",
    "        path = '../input/glove840b300dtxt/glove.840B.300d.txt'\n",
    "    elif CONFIG['embedding_mode'] == 'glove6b50d':\n",
    "        path = '../input/nlpword2vecembeddingspretrained/glove.6B.50d.txt'\n",
    "    elif CONFIG['embedding_mode'] == 'glove6b100d':\n",
    "        path = '../input/nlpword2vecembeddingspretrained/glove.6B.100d.txt'\n",
    "    elif CONFIG['embedding_mode'] == 'glove6b200d':\n",
    "        path = '../input/nlpword2vecembeddingspretrained/glove.6B.200d.txt'\n",
    "    \n",
    "    # load glove\n",
    "    f = open(path)\n",
    "    for line in tqdm(f):\n",
    "        value = line.split(' ')\n",
    "        word = value[0]\n",
    "        coef = np.array(value[1:],dtype = 'float32')\n",
    "        embedding_vectors[word] = coef\n",
    "        \n",
    "    # create matrices for tokenizers\n",
    "    input_emb_matrix = create_emb_matrix(input_tokenizer, embedding_vectors)\n",
    "    target_emb_matrix = create_emb_matrix(target_tokenizer, embedding_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(length, depth):\n",
    "    depth = depth/2\n",
    "\n",
    "    positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
    "    depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
    "\n",
    "    angle_rates = 1 / (10000**depths)         # (1, depth)\n",
    "    angle_rads = positions * angle_rates      # (pos, depth)\n",
    "\n",
    "    pos_encoding = np.concatenate(\n",
    "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "      axis=-1) \n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "# We try out the encoding func\n",
    "pos_encoding = positional_encoding(length=2048, depth=512)\n",
    "print('Positional encoding shape', pos_encoding.shape) # Check the shape.\n",
    "\n",
    "# Plot the dimensions.\n",
    "plt.pcolormesh(pos_encoding.numpy().T, cmap='RdBu')\n",
    "plt.ylabel('Depth')\n",
    "plt.xlabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model, emb_matrix):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        if emb_matrix is None:\n",
    "            self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
    "        else:\n",
    "            self.embedding = tf.keras.layers.Embedding(\n",
    "            vocab_size,\n",
    "            d_model,\n",
    "            weights=[emb_matrix],\n",
    "            trainable=CONFIG['embeddings_trainable'],\n",
    "            mask_zero=True\n",
    "        )\n",
    "        \n",
    "        self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
    "\n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        return self.embedding.compute_mask(*args, **kwargs)\n",
    "\n",
    "    def call(self, x):\n",
    "        length = tf.shape(x)[1]\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # This factor sets the relative scale of the embedding and positonal_encoding.\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
    "        return x \n",
    "\n",
    "# Try\n",
    "# sample_emb_input = TransformerEmbedding(vocab_size=input_tokenizer.vocab_size, d_model=512)(sample_input)\n",
    "# sample_emb_target = TransformerEmbedding(vocab_size=target_tokenizer.vocab_size, d_model=512)(sample_target)\n",
    "\n",
    "# sample_emb_target._keras_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base attention for further subclassing\n",
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(BaseAttention):\n",
    "    \n",
    "    # x = target sequence, context = context sequence\n",
    "    def call(self, x, context):\n",
    "        attn_output, attn_scores = self.mha(\n",
    "            query=x,\n",
    "            key=context,\n",
    "            value=context,\n",
    "            return_attention_scores=True)\n",
    "\n",
    "        # Cache the attention scores for plotting later.\n",
    "        self.last_attn_scores = attn_scores\n",
    "\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Try on sample\n",
    "# sample_ca = CrossAttention(num_heads=2, key_dim=512)\n",
    "# print(sample_emb_input.shape)\n",
    "# print(sample_emb_target.shape)\n",
    "# print(sample_ca(sample_emb_input, sample_emb_target).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalSelfAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x)\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "\n",
    "# Try\n",
    "# sample_gsa = GlobalSelfAttention(num_heads=2, key_dim=512)\n",
    "# print(sample_emb_input.shape)\n",
    "# print(sample_gsa(sample_emb_input).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x,\n",
    "            use_causal_mask = True)\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "    \n",
    "# sample_csa = CausalSelfAttention(num_heads=2, key_dim=512)\n",
    "# print(sample_emb_target.shape)\n",
    "# print(sample_csa(sample_emb_target).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test latter elements do not depend on earlier elements, making no difference\n",
    "# if we remove the earlier elements before or after applying csa layer\n",
    "# out1 = sample_csa(embed_target(sample_target[:, :3])) \n",
    "# out2 = sample_csa(embed_target(sample_target))[:, :3]\n",
    "# tf.reduce_max(abs(out1 - out2)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.seq = tf.keras.Sequential([\n",
    "          tf.keras.layers.Dense(dff, activation='relu'),\n",
    "          tf.keras.layers.Dense(d_model),\n",
    "          tf.keras.layers.Dropout(dropout_rate)\n",
    "        ])\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.add([x, self.seq(x)])\n",
    "        x = self.layer_norm(x) \n",
    "        return x\n",
    "\n",
    "# Try\n",
    "# sample_ffn = FeedForward(512, 2048)\n",
    "# print(sample_emb_target.shape)\n",
    "# print(sample_ffn(sample_emb_target).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attention = GlobalSelfAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "\n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.self_attention(x)\n",
    "        x = self.ffn(x)\n",
    "        return x\n",
    "    \n",
    "# sample_encoder_layer = EncoderLayer(d_model=512, num_heads=8, dff=2048)\n",
    "# print(sample_encoder_layer(sample_emb_input).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads,\n",
    "                   dff, vocab_size, dropout_rate=0.1):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.pos_embedding = TransformerEmbedding(\n",
    "            vocab_size=vocab_size, d_model=d_model,\n",
    "            emb_matrix=input_emb_matrix,\n",
    "        )\n",
    "\n",
    "        self.enc_layers = [\n",
    "            EncoderLayer(d_model=d_model,\n",
    "                         num_heads=num_heads,\n",
    "                         dff=dff,\n",
    "                         dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        \n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x):\n",
    "        # `x` is token-IDs shape: (batch_size, seq_len)\n",
    "        x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "\n",
    "        # Add dropout.\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x)\n",
    "\n",
    "        return x  # Shape `(batch_size, seq_len, d_model)`.\n",
    "\n",
    "# Try\n",
    "# sample_encoder = Encoder(num_layers=4,\n",
    "#                          d_model=512,\n",
    "#                          num_heads=8,\n",
    "#                          dff=2048,\n",
    "#                          vocab_size=input_tokenizer.vocab_size)\n",
    "\n",
    "# sample_encoder_output = sample_encoder(sample_input, training=False)\n",
    "\n",
    "# print(sample_input.shape)\n",
    "# print(sample_encoder_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, *, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.causal_self_attention = CausalSelfAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "\n",
    "        self.cross_attention = CrossAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "\n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x, context):\n",
    "        x = self.causal_self_attention(x=x)\n",
    "        x = self.cross_attention(x=x, context=context)\n",
    "\n",
    "        # Cache the last attention scores for plotting later\n",
    "        self.last_attn_scores = self.cross_attention.last_attn_scores\n",
    "\n",
    "        x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "        return x\n",
    "    \n",
    "# sample_decoder_layer = DecoderLayer(d_model=512, num_heads=8, dff=2048)\n",
    "# sample_decoder_layer_output = sample_decoder_layer(\n",
    "#     x=sample_emb_target, context=sample_emb_input\n",
    "# )\n",
    "\n",
    "# print(sample_emb_input.shape)\n",
    "# print(sample_emb_target.shape)\n",
    "# print(sample_decoder_layer_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n",
    "               dropout_rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.pos_embedding = TransformerEmbedding(vocab_size=vocab_size,\n",
    "                                                 d_model=d_model,\n",
    "                                                 emb_matrix=target_emb_matrix\n",
    "                                                )\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dec_layers = [\n",
    "            DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
    "                         dff=dff, dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)]\n",
    "\n",
    "        self.last_attn_scores = None\n",
    "\n",
    "    def call(self, x, context):\n",
    "        # `x` is token-IDs shape (batch, target_seq_len)\n",
    "        x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x  = self.dec_layers[i](x, context)\n",
    "\n",
    "        self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
    "\n",
    "        # The shape of x is (batch_size, target_seq_len, d_model).\n",
    "        return x\n",
    "\n",
    "# Try\n",
    "# sample_decoder = Decoder(num_layers=4,\n",
    "#                          d_model=512,\n",
    "#                          num_heads=8,\n",
    "#                          dff=2048,\n",
    "#                          vocab_size=target_tokenizer.vocab_size)\n",
    "\n",
    "# sample_decoder_output = sample_decoder(x=sample_target, context=sample_emb_input)\n",
    "\n",
    "# print(sample_target.shape)\n",
    "# print(sample_emb_input.shape)\n",
    "# print(sample_decoder_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_decoder.last_attn_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
    "               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
    "                               num_heads=num_heads, dff=dff,\n",
    "                               vocab_size=input_vocab_size,\n",
    "                               dropout_rate=dropout_rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
    "                               num_heads=num_heads, dff=dff,\n",
    "                               vocab_size=target_vocab_size,\n",
    "                               dropout_rate=dropout_rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # To use a Keras model with `.fit` you must pass all your inputs in the\n",
    "        # first argument.\n",
    "        context, x  = inputs\n",
    "\n",
    "        context = self.encoder(context)  # (batch_size, context_len, d_model)\n",
    "\n",
    "        x = self.decoder(x, context)  # (batch_size, target_len, d_model)\n",
    "\n",
    "        # Final linear layer output.\n",
    "        logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
    "\n",
    "        try:\n",
    "            # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
    "            # b/250038731\n",
    "            del logits._keras_mask\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        # Return the final output and the attention weights.\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct transformer\n",
    "transformer = Transformer(\n",
    "    num_layers=CONFIG['num_layers'],\n",
    "    d_model=CONFIG['d_model'],\n",
    "    num_heads=CONFIG['num_heads'],\n",
    "    dff=CONFIG['dff'],\n",
    "    input_vocab_size=input_tokenizer.vocab_size,\n",
    "    target_vocab_size=target_tokenizer.vocab_size,\n",
    "    dropout_rate=CONFIG['dropout_rate']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokenizer.vocab_size, target_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call transformer on sample input, this will build it and setup inputs\n",
    "\n",
    "sample_transformer_output = transformer((sample_input, sample_target))\n",
    "print(sample_input.shape)\n",
    "print(sample_target.shape)\n",
    "print(sample_transformer_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_transformer_attn_scores = transformer.decoder.dec_layers[-1].last_attn_scores\n",
    "# print(sample_transformer_attn_scores.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, dtype=tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'd_model': self.d_model,\n",
    "            'warmup_steps': self.warmup_steps\n",
    "        }\n",
    "        return config\n",
    "#         base_config = super(CustomSchedule, self).get_config()\n",
    "#         return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "learning_rate = CustomSchedule(CONFIG['d_model'], CONFIG['learning_rate_warmup_steps'])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)\n",
    "\n",
    "\n",
    "plt.plot(learning_rate(tf.range(40000, dtype=tf.float32)))\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.xlabel('Train Step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(label, pred):\n",
    "    mask = label != 0\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none'\n",
    "    )\n",
    "    loss = loss_object(label, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "\n",
    "    loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def masked_accuracy(label, pred):\n",
    "    pred = tf.argmax(pred, axis=2)\n",
    "    label = tf.cast(label, pred.dtype)\n",
    "    match = label == pred\n",
    "\n",
    "    mask = label != 0\n",
    "\n",
    "    match = match & mask\n",
    "\n",
    "    match = tf.cast(match, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(match)/tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "# masked_loss(tf.constant([0.5], dtype=tf.float32), tf.constant([[0.5, 0.2]], dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.compile(\n",
    "    loss=masked_loss,\n",
    "    optimizer=optimizer,\n",
    "    metrics=[masked_accuracy]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks \n",
    "!mkdir -p checkpoints\n",
    "\n",
    "modeldir = f\"checkpoints\"\n",
    "checkpoint_filepath = modeldir + '/checkpoint.hdf'\n",
    "print('Model checkpoint:', checkpoint_filepath)\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    mode='min', verbose=1,\n",
    "    patience=CONFIG['early_stopping_patience'],\n",
    "    restore_best_weights=True # restore only best weights relative to val_loss\n",
    ")\n",
    "\n",
    "csv_logger=tf.keras.callbacks.CSVLogger(\n",
    "    modeldir + '/log.csv', separator=\",\", append=True\n",
    ")\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "try:\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "    print('Loaded model weights checkpoint.')\n",
    "except:\n",
    "    print('Cannot load model weights from checkpoint, it may not exist yet.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_run = wandb.init(\n",
    "    project=CONFIG['wandb_project'], \n",
    "    config=CONFIG,\n",
    "    group=CONFIG['wandb_group'], \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Training\n",
    "    \n",
    "history = transformer.fit(\n",
    "    dataset_train,\n",
    "    epochs=CONFIG['max_epochs'],\n",
    "    validation_data=dataset_val,\n",
    "    callbacks=[\n",
    "        WandbCallback(save_model=False),\n",
    "        early_stopping,\n",
    "        csv_logger,\n",
    "        model_checkpoint_callback\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    fig, (axl, axa) = plt.subplots(nrows=2, ncols=1)\n",
    "    axl.plot(history.history['loss'], label='loss')\n",
    "    axl.plot(history.history['val_loss'], label='val_loss')\n",
    "    axl.set_ylim([0, 10])\n",
    "    axl.set_xlabel('Epoch')\n",
    "    axl.set_ylabel('Loss')\n",
    "    axl.legend()\n",
    "    axl.grid(True)\n",
    "    \n",
    "    axa.plot(history.history['masked_accuracy'], label='masked_accuracy')\n",
    "    axa.plot(history.history['val_masked_accuracy'], label='val_masked_accuracy')\n",
    "    axa.set_ylim([0, 1])\n",
    "    axa.set_xlabel('Epoch')\n",
    "    axa.set_ylabel('Accuracy')\n",
    "    axa.legend()\n",
    "    axa.grid(True)\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "if history != None:\n",
    "    plot_history(history)\n",
    "else:\n",
    "    print(\"No history to display.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEGACY - unoptimized inference using python array\n",
    "class SummarizerSingleStep(tf.Module):\n",
    "    def __init__(self, transformer, input_tokenizer, target_tokenizer):\n",
    "        \n",
    "        # todo\n",
    "        self.transformer = transformer\n",
    "        self.input_tokenizer = input_tokenizer\n",
    "        self.target_tokenizer = target_tokenizer\n",
    "\n",
    "    # expect sentence to be prepared with <sos> <eos> and clean\n",
    "    # @tf.function(input_signature=[tf.TensorSpec(dtype=tf.string, shape=[None])])\n",
    "    def __call__(self, sentence: str):\n",
    "\n",
    "        encoder_input = tf.expand_dims(self.input_tokenizer.vectorizer(sentence), 0)\n",
    "\n",
    "        # `tf.TensorArray` is required here (instead of a Python list), so that the\n",
    "        # dynamic-loop can be traced by `tf.function`.\n",
    "        # output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "        # output_array = output_array.write(0, summary_start_token)\n",
    "        \n",
    "        summary_start_token = target_tokenizer.vectorizer('<sos>')[0].numpy()\n",
    "        summary_end_token = target_tokenizer.vectorizer('<eos>')[0].numpy()\n",
    "        \n",
    "        decoder_input = [summary_start_token]\n",
    "        output = tf.expand_dims(decoder_input, 0)\n",
    "\n",
    "        for i in tf.range(self.target_tokenizer.max_length):\n",
    "            \n",
    "            predictions = self.transformer([encoder_input, output], training=False)\n",
    "\n",
    "            # Select the last token from the `seq_len` dimension.\n",
    "            predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n",
    "\n",
    "            # TODO: why cast needed here ??\n",
    "            predicted_id = tf.cast(tf.argmax(predictions, axis=-1), dtype=tf.int32)\n",
    "\n",
    "            # Concatenate the `predicted_id` to the output which is given to the\n",
    "            # decoder as its input.\n",
    "            # output_array = output_array.write(i+1, predicted_id[0])\n",
    "            output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "            if predicted_id == summary_end_token:\n",
    "                break\n",
    "        \n",
    "        \n",
    "        prediction = tf.squeeze(output, axis=0)\n",
    "        tokens = np.expand_dims(prediction.numpy(), 0)\n",
    "        \n",
    "        # print(tokens)\n",
    "        \n",
    "        # text = self.tokenization.summary_tokenizer.sequences_to_texts(tokens)[0]\n",
    "        text = self.target_tokenizer.tokens_to_text(tokens)[0]\n",
    "\n",
    "        # `tf.function` prevents us from using the attention_weights that were\n",
    "        # calculated on the last iteration of the loop.\n",
    "        # So, recalculate them outside the loop.\n",
    "        self.transformer([encoder_input, output[:,:-1]], training=False)\n",
    "        attention_weights = self.transformer.decoder.last_attn_scores\n",
    "\n",
    "        return text, tokens, attention_weights\n",
    "    \n",
    "# def str_to_tf(self, text: str):\n",
    "#         return tf.constant([text])\n",
    "    \n",
    "# def str_from_tf(self, tensor: tf.Tensor):\n",
    "#     return bytes.decode(tensor.numpy())\n",
    "\n",
    "# Optimized inference module using tensorflow tensorarray, saveable as TF graph saved model\n",
    "class SummarizerModule(tf.Module):\n",
    "    def __init__(self, transformer, input_tokenizer, target_tokenizer):\n",
    "        \n",
    "        self.transformer = transformer\n",
    "        self.input_tokenizer = input_tokenizer\n",
    "        self.target_tokenizer = target_tokenizer\n",
    "\n",
    "    # TF func inference prediction\n",
    "    # expect sentence to be prepared with <sos> <eos> and clean\n",
    "    @tf.function(input_signature=[tf.TensorSpec(dtype=tf.string, shape=[None])])\n",
    "    def predict(self, sentence: tf.Tensor):\n",
    "\n",
    "        encoder_input = self.input_tokenizer.vectorizer(sentence)\n",
    "        \n",
    "        start_token = target_tokenizer.vectorizer(tf.constant('<sos>'))[0][tf.newaxis]\n",
    "        end_token = target_tokenizer.vectorizer(tf.constant('<eos>'))[0][tf.newaxis]\n",
    "        \n",
    "        # `tf.TensorArray` is required here (instead of a Python list), so that the\n",
    "        # dynamic-loop can be traced by `tf.function`.\n",
    "        output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "        output_array = output_array.write(0, start_token)\n",
    "        output = tf.transpose(output_array.stack())\n",
    "\n",
    "        for i in tf.range(self.target_tokenizer.max_length):\n",
    "            \n",
    "            output = tf.transpose(output_array.stack())\n",
    "            \n",
    "            predictions = self.transformer([encoder_input, output], training=False)\n",
    "\n",
    "            # Select the last token from the `seq_len` dimension.\n",
    "            predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n",
    "\n",
    "            # argmax\n",
    "            predicted_id = tf.argmax(predictions, axis=-1)\n",
    "\n",
    "            # Concatenate the `predicted_id` to the output which is given to the\n",
    "            # decoder as its input.\n",
    "            # output_array = output_array.write(i+1, predicted_id[0])\n",
    "            output_array = output_array.write(i+1, predicted_id[0])\n",
    "\n",
    "            # stop on end\n",
    "            if predicted_id == end_token:\n",
    "                break\n",
    "        \n",
    "        output = tf.transpose(output_array.stack())\n",
    "        \n",
    "        # print(tokens)\n",
    "        \n",
    "        # text = self.tokenization.summary_tokenizer.sequences_to_texts(tokens)[0]\n",
    "        text = self.target_tokenizer.tokens_to_text(output)[0]\n",
    "\n",
    "        # `tf.function` prevents us from using the attention_weights that were\n",
    "        # calculated on the last iteration of the loop.\n",
    "        # So, recalculate them outside the loop.\n",
    "        self.transformer([encoder_input, output[:,:-1]], training=False)\n",
    "        attention_weights = self.transformer.decoder.last_attn_scores\n",
    "\n",
    "        return text, output, attention_weights\n",
    "\n",
    "summarizer_module = SummarizerModule(transformer, input_tokenizer, target_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# (inference benchmark)\n",
    "\n",
    "# summarizer_single = SummarizerSingleStep(transformer, input_tokenizer, target_tokenizer)\n",
    "# df_train[:5]['article'].apply(lambda text: summarizer_single(text)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# df_train[:5]['article'].apply(lambda text: summarizer(summarizer.to_tf(text))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_tokens(text):\n",
    "        text = text.lower()\n",
    "        text = text.replace(\"<sos>\", \"\").replace(\"<eos>\", \"\")\n",
    "        text = text.replace(\"<unk>\", \"##\")\n",
    "        text = text.replace(\"<dot>\", \". \") # normal syntax with dot at end\n",
    "        text = text.strip()\n",
    "        return text\n",
    "\n",
    "def remove_special_tokens_frame(frame: pd.DataFrame):\n",
    "    frame['article'] = frame['article'].apply(remove_special_tokens)\n",
    "    frame['summary'] = frame['summary'].apply(remove_special_tokens)\n",
    "    frame['predicted'] = frame['predicted'].apply(remove_special_tokens)\n",
    "    return frame\n",
    "    \n",
    "# Run summarization inference on entire frame\n",
    "def summarize_frame(frame):\n",
    "    \n",
    "    frame = frame.copy()\n",
    "    frame['predicted'] = '<NONE>'\n",
    "    \n",
    "    for i in range(0, frame.shape[0]):\n",
    "        if i%100 == 0:\n",
    "            print(f\"Summarising ... {i}/{frame.shape[0]}\")\n",
    "            \n",
    "        article = frame.iloc[i]['article']\n",
    "        summary = frame.iloc[i]['summary']\n",
    "        \n",
    "        summarized_tf, summarized_tokens, attention_weights = summarizer_module.predict(\n",
    "            tf.constant([article])\n",
    "        )\n",
    "        \n",
    "        summarized_text = bytes.decode(summarized_tf.numpy())\n",
    "        \n",
    "        frame.iloc[i, frame.columns.get_loc('predicted')] = summarized_text\n",
    "        \n",
    "    return frame\n",
    "\n",
    "def pretty_summaries(frame):\n",
    "    \n",
    "    for i, row in frame.iterrows():\n",
    "        print(f\"\\n ------------------\")\n",
    "        print(f\"Article  : {remove_special_tokens(row['article'])}\")\n",
    "        print(f\"\\nSummary  : {remove_special_tokens(row['summary'])}\")\n",
    "        print(f\"\\nPredicted: {remove_special_tokens(row['predicted'])}\")\n",
    "        print()\n",
    "        print(f\"------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text processing module from pmxy-stranasum-preprocessing used as summarizer module component\n",
    "class TextProcessor:\n",
    "    \n",
    "    # Text cleanup\n",
    "    def clean_text(self, text: str):\n",
    "\n",
    "        # lowercase\n",
    "        text = str(text).lower()\n",
    "\n",
    "        # remove &-escaped characters\n",
    "        text = re.sub(r\"&.[1-9]+;\",\" \", str(text))\n",
    "\n",
    "        # remove escaped characters\n",
    "        text=re.sub(\"(\\\\t)\", ' ', str(text))\n",
    "        text=re.sub(\"(\\\\r)\", ' ', str(text))\n",
    "        text=re.sub(\"(\\\\n)\", ' ', str(text))\n",
    "\n",
    "        # remove double characters\n",
    "        text=re.sub(\"(__+)\", ' ', str(text))  #remove _ if it occurs more than one time consecutively\n",
    "        text=re.sub(\"(--+)\", ' ', str(text))   #remove - if it occurs more than one time consecutively\n",
    "        text=re.sub(\"(~~+)\", ' ', str(text))   #remove ~ if it occurs more than one time consecutively\n",
    "        text=re.sub(\"(\\+\\++)\", ' ', str(text))  #remove + if it occurs more than one time consecutively\n",
    "        text=re.sub(\"(\\.\\.+)\", ' ', str(text))  #remove . if it occurs more than one time consecutively\n",
    "        \n",
    "        # special - fix u.s. contraction in gigaword\n",
    "        text = re.sub(\"(u\\.s\\.)\", 'united states', str(text))\n",
    "        \n",
    "        # fix contractions to base form\n",
    "        text = contractions.fix(text)\n",
    "\n",
    "        #remove special tokens <>()|&\"',;?~*!\n",
    "        text=re.sub(r\"[<>()|&\\[\\]\\'\\\",;?~*!]\", ' ', str(text)).lower()\n",
    "\n",
    "        # CNN mail data cleanup\n",
    "        text=re.sub(\"(mailto:)\", ' ', str(text)) #remove mailto:\n",
    "        text=re.sub(r\"(\\\\x9\\d)\", ' ', str(text)) #remove \\x9* in text\n",
    "        text=re.sub(\"([iI][nN][cC]\\d+)\", 'INC_NUM', str(text)) #replace INC nums to INC_NUM\n",
    "        text=re.sub(\"([cC][mM]\\d+)|([cC][hH][gG]\\d+)\", 'CM_NUM', str(text)) #replace CM# and CHG# to CM_NUM\n",
    "\n",
    "        # url replacement into base form\n",
    "        try:\n",
    "            url = re.search(r'((https*:\\/*)([^\\/\\s]+))(.[^\\s]+)', str(text))\n",
    "            repl_url = url.group(3)\n",
    "            text = re.sub(r'((https*:\\/*)([^\\/\\s]+))(.[^\\s]+)',repl_url, str(text))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "        # handle dot at the end of words\n",
    "        text=re.sub(\"(\\.\\s+)\", ' ', str(text)) # remove\n",
    "        \n",
    "        text=re.sub(\"(\\-\\s+)\", ' ', str(text)) #remove - at end of words(not between)\n",
    "        text=re.sub(\"(\\:\\s+)\", ' ', str(text)) #remove : at end of words(not between)\n",
    "\n",
    "        #remove multiple spaces\n",
    "        text=re.sub(\"(\\s+)\",' ',str(text))\n",
    "\n",
    "        # apply lowercase again\n",
    "        text = text.lower().strip()\n",
    "        \n",
    "        # remove trailing dot, we will apply end of sequence anyway\n",
    "        text = re.sub(\"(\\.)$\", '', str(text)).strip()\n",
    "        \n",
    "        # gigaword - UNK token\n",
    "        text = re.sub(\"unk\", '', str(text).strip())\n",
    "        \n",
    "        # gigaword - change numbers to hashtags\n",
    "        text = re.sub(\"\\d\", \"#\", str(text).strip())\n",
    "\n",
    "        return text\n",
    "\n",
    "    def apply_special_tokens(self, text):\n",
    "        text = str(text).strip()\n",
    "        text = \"<sos> \" + str(text).strip() + \" <eos>\"\n",
    "        return text\n",
    "\n",
    "    def remove_special_tokens(self, text):\n",
    "        text = text.lower()\n",
    "        text = text.replace(\"<sos>\", \"\").replace(\"<eos>\", \"\")\n",
    "        text = text.strip()\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summarization class using loadable TF graph model an processing\n",
    "class Summarizer:\n",
    "    \n",
    "    # Initialize using SummarizationModule or loaded graph tf module\n",
    "    def __init__(self, module: tf.Module):\n",
    "        self.module = module\n",
    "        self.processor = TextProcessor()\n",
    "        \n",
    "    def summarize(self, text: str):\n",
    "        prepared = self.processor.apply_special_tokens(self.processor.clean_text(text))\n",
    "        output_text, output_tensor, weights = self.module.predict(tf.constant([prepared]))\n",
    "        return prepared, self.processor.remove_special_tokens(bytes.decode(output_text.numpy())), output_tensor, weights\n",
    "    \n",
    "    # Shorthand for text output only\n",
    "    def __call__(self, text: str):\n",
    "        return self.summarize(text)[1]\n",
    "    \n",
    "summarizer = Summarizer(summarizer_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference on test and val sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Predict on test set and save\n",
    "print('--- Runing inference on test set ---\\n')\n",
    "test_pred = remove_special_tokens_frame(summarize_frame(df_test))\n",
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred.to_csv('testset_evaluation_data.csv')\n",
    "print('Saved test set evaluation data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Predict on validation set with same size as test set and save\n",
    "print('--- Runing inference on validation set ---\\n')\n",
    "val_pred = remove_special_tokens_frame(summarize_frame(df_val[:df_test.shape[0]]))\n",
    "val_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred.to_csv('validationset_evaluation_data.csv')\n",
    "print('Saved validation set evaluation data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--- Example 10 test set summaries ---\\n')\n",
    "pretty_summaries(test_pred[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--- Example 10 validation set summaries ---\\n')\n",
    "pretty_summaries(val_pred[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "summarizer('An 18 year old man joe mama has been killed in a tragic car accident in washington us the police have reported that a car has crashed with a lorry this evening on new york street the injured man has died on site.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "summarizer('The us president Joe Biden has annouced that he will be coming to china in april to consult the chinese president xi jingping about the current nuclear threats.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# note, will fail on empty seq\n",
    "bleu = evaluate.load('bleu')\n",
    "bleu_test = bleu.compute(references=test_pred['summary'], predictions=test_pred['predicted'])\n",
    "bleu_val = bleu.compute(references=val_pred['summary'], predictions=val_pred['predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load('rouge')\n",
    "rouge_test = rouge.compute(references=test_pred['summary'], predictions=test_pred['predicted'])\n",
    "rouge_val = rouge.compute(references=val_pred['summary'], predictions=val_pred['predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate metrics after inference and save to unified frame, save to \n",
    "\n",
    "ev_metrics = {\n",
    "    'loss_best': min(history.history['loss']),\n",
    "    'masked_accuracy_best': max(history.history['masked_accuracy']),\n",
    "    'val_loss_best': min(history.history['val_loss']),\n",
    "    'val_masked_accuracy_best': max(history.history['val_masked_accuracy']),\n",
    "    \n",
    "    'test_rouge1': rouge_test['rouge1'],\n",
    "    'test_rouge2': rouge_test['rouge2'],\n",
    "    'test_rougeL': rouge_test['rougeL'],\n",
    "    'test_rougeLsum': rouge_test['rougeLsum'],\n",
    "    'test_bleu': bleu_test['bleu'],\n",
    "    \n",
    "    'val_rouge1': rouge_val['rouge1'],\n",
    "    'val_rouge2': rouge_val['rouge2'],\n",
    "    'val_rougeL': rouge_val['rougeL'],\n",
    "    'val_rougeLsum': rouge_val['rougeLsum'],\n",
    "    'val_bleu': bleu_val['bleu'],\n",
    "}\n",
    "\n",
    "pd.DataFrame(ev_metrics, index=[0]).to_csv('evaluation_metrics.csv')\n",
    "\n",
    "print(\"Evaluation metrics: \");\n",
    "pprint(ev_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to wandb metric summary\n",
    "for metric in ev_metrics:\n",
    "    wandb_run.summary[metric] = ev_metrics[metric]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save summarization module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.saved_model.save(summarizer_module, 'summarizer_module')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_summarizer = Summarizer(tf.saved_model.load('summarizer_module'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_summarizer('London\\'s police force has failed to learn enough from its failures in a 2016 serial killer case to stop similar crimes happening again, a police watchdog said on Thursday in a damning report.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dir as artifact\n",
    "artifact = wandb.Artifact('artifact', type='model')\n",
    "artifact.add_dir('.')\n",
    "wandb_run.log_artifact(artifact)\n",
    "\n",
    "# finish wandb run\n",
    "wandb_run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
